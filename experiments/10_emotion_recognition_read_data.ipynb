{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47616784]\n",
      "[0.47616783]\n",
      "[0.476]\n",
      "[1.36337572e-08]\n",
      "[9.36227284e-05]\n"
     ]
    }
   ],
   "source": [
    "# @title = \"Error introduced by float32 and float16 from float64\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "number = np.random.rand(1)\n",
    "\n",
    "\n",
    "print(number)\n",
    "print(np.array(number, dtype=np.float32))\n",
    "print(np.array(number, dtype=np.float16))\n",
    "\n",
    "\n",
    "print(number-np.array(number, dtype=np.float32))\n",
    "print(number-np.array(number, dtype=np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./emotion_recognition_data/sub-34/eeg/sub-34_task-ImaginedEmotion_eeg.set',\n",
       " './emotion_recognition_data/sub-35/eeg/sub-35_task-ImaginedEmotion_eeg.set']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob.glob('./emotion_recognition_data/*/eeg/sub-*_task-ImaginedEmotion_eeg.set'))[32:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/rickbook/document/neuro/project-abns/project-abns/experiments/emotion_recognition_data/sub-34/eeg/sub-34_task-ImaginedEmotion_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26638/104442116.py:132: RuntimeWarning: Estimated head radius (0.0 cm) is below the 3rd percentile for infant head size. Check if the montage_units argument is correct (the default is \"mm\", but your channel positions may be in different units).\n",
      "  raw = mne.io.read_raw_eeglab(eeglab_file, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 1108223  =      0.000 ...  4328.996 secs...\n",
      "Used Annotations descriptions: ['anger', 'awe', 'compassion', 'content', 'disgust', 'excite', 'fear', 'frustration', 'grief', 'happy', 'jealousy', 'joy', 'love', 'relief', 'sad']\n",
      "Not setting metadata\n",
      "15 matching events found\n",
      "Setting baseline interval to [-10.0, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 15 events and 25601 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 845 samples (3.301 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 2700 out of 2700 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED: ./data/emotion_recognition_preprocessed_data/data_sub-34.npz\n",
      "Reading /home/rickbook/document/neuro/project-abns/project-abns/experiments/emotion_recognition_data/sub-35/eeg/sub-35_task-ImaginedEmotion_eeg.fdt\n",
      "Reading 0 ... 1511167  =      0.000 ...  5902.996 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26638/104442116.py:132: RuntimeWarning: Estimated head radius (0.0 cm) is below the 3rd percentile for infant head size. Check if the montage_units argument is correct (the default is \"mm\", but your channel positions may be in different units).\n",
      "  raw = mne.io.read_raw_eeglab(eeglab_file, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['anger', 'awe', 'compassion', 'content', 'disgust', 'excite', 'fear', 'frustration', 'grief', 'happy', 'jealousy', 'joy', 'love', 'relief', 'sad']\n",
      "Not setting metadata\n",
      "15 matching events found\n",
      "Setting baseline interval to [-10.0, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 15 events and 25601 original time points ...\n",
      "0 bad epochs dropped\n",
      "Applying baseline correction (mode: mean)\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 845 samples (3.301 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 3120 out of 3120 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED: ./data/emotion_recognition_preprocessed_data/data_sub-35.npz\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# list files\n",
    "files_name_eeglab = sorted(glob.glob('./emotion_recognition_data/*/eeg/sub-*_task-ImaginedEmotion_eeg.set'))[32:]\n",
    "files_name_tsv = sorted(glob.glob('./emotion_recognition_data/*/eeg/sub-*_task-ImaginedEmotion_events.tsv'))[32:]\n",
    "\n",
    "\"\"\" Experiment setup for the EEG emotion recognition study.\n",
    "\n",
    "    source: https://openneuro.org/datasets/ds003004/versions/1.1.1\n",
    "\n",
    "    PARADIGM: The study uses the method of guided imagery to induce resting, eyes-closed \n",
    "    participants using voice-guided imagination to enter distinct 15 emotion states during \n",
    "    acquisition of high-density EEG data.\n",
    "\n",
    "    During the study, participants listen to 15 voice recordings that each suggest \n",
    "    imagining a scenario in which they have experienced -- or would experience the named \n",
    "    target emotion. Some target emotions have positive valence (e.g., joy, happiness), \n",
    "    others negative valence (e.g., sadness, anger). Before and between the 15 emotion \n",
    "    imagination periods, participants hear relaxation suggestions ('Now return to a neutral \n",
    "    state by ...').\n",
    "\n",
    "    PROCEDURE: When the participant first begins to feel the target emotion, they are asked \n",
    "    to indicate this by pressing a handheld button. Participants are asked to continue \n",
    "    feeling the emotion as long as possible. To intensify and lengthen the periods of experienced \n",
    "    emotion, participants are asked to interoceptively perceive and attend relevant somatosensory \n",
    "    sensations. When the target feeling wanes (typically after 1 and 5 minutes), participants push \n",
    "    the button again to leave the emotion imagination period and cue the relaxation instructions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pd.read_csv('./emotion_recognition_data/sub-01/eeg/sub-01_task-ImaginedEmotion_events.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "#######################################################\n",
    "######## LOAD THE ANNONTATIONS FROM THE TSV FILE ######\n",
    "#######################################################\n",
    "\n",
    "list_of_emotions = [ \n",
    "                    'awe', \n",
    "                    'frustration',\n",
    "                    'joy',\n",
    "                    'anger',\n",
    "                    'happy',\n",
    "                    'sad',\n",
    "                    'love',\n",
    "                    'fear',\n",
    "                    'compassion',\n",
    "                    'jealousy',\n",
    "                    'content',\n",
    "                    'grief',\n",
    "                    'relief',\n",
    "                    'excite',\n",
    "                    'disgust'\n",
    "                    ]\n",
    "\n",
    "press_button = 'press1'\n",
    "\n",
    "\n",
    "def get_emotion_onset(tsv_file, duration = 100.0):\n",
    "    \"\"\" Get the onset of the emotion from the tsv file.\n",
    "\n",
    "    Args:\n",
    "        @tsv_file: the tsv file with the emotion data\n",
    "        @duration: the duration of the emotion in seconds\n",
    "\n",
    "    Returns:\n",
    "        @df: a dataframe with the emotion, the onset and the duration\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(tsv_file, sep='\\t')\n",
    "\n",
    "    emotions = []\n",
    "    onsents = []\n",
    "\n",
    "    # we need to check if the next value is press1\n",
    "    # when the person press1, means that he/she is \n",
    "    # feeling the emotion\n",
    "    for idx, i in enumerate(zip(df['value'], df['onset'])):\n",
    "\n",
    "        # check if the value is in the list of emotions\n",
    "        if i[0] in list_of_emotions:  \n",
    "            # check if the next value is press1\n",
    "            if df['value'][idx+1] == press_button:\n",
    "\n",
    "                # print(df['value'][idx], df['value'][idx+1], df['onset'][idx+1])\n",
    "\n",
    "                # append the emotion\n",
    "                emotions.append(df['value'][idx])\n",
    "\n",
    "                # append the onset\n",
    "                onsents.append(df['onset'][idx+1])\n",
    "\n",
    "    # create a dataframe\n",
    "    df = pd.DataFrame({'value': emotions, 'onset': onsents, 'duration': [duration]*len(emotions)})\n",
    "\n",
    "    return df\n",
    "\n",
    "#######################################################\n",
    "######## LOAD EEG DATA FROM THE EEG FILE ##############\n",
    "#######################################################\n",
    "\n",
    "# load the data\n",
    "def get_data_eeglab(files_name_eeglab, files_name_tsv, base_path):\n",
    "    \"\"\" This function loads the data from the eeglab files and the tsv \n",
    "    files and saves them in a npz file.\n",
    "\n",
    "    @param files_name_eeglab: list of strings\n",
    "        list of the eeglab files\n",
    "    @param files_name_tsv: list of strings\n",
    "        list of the tsv files\n",
    "    @param base_path: string\n",
    "        path where to save the data\n",
    "\n",
    "    @return: None\n",
    "    \"\"\"\n",
    "    files_name = zip(files_name_eeglab, files_name_tsv)\n",
    "\n",
    "    data = []\n",
    "    for eeglab_file, tsv in files_name:\n",
    "\n",
    "        #######################################################\n",
    "        ######## LOAD EEG DATA FROM THE EEG FILE ##############\n",
    "        #######################################################\n",
    "        # load the data\n",
    "        raw = mne.io.read_raw_eeglab(eeglab_file, preload=True)\n",
    "\n",
    "\n",
    "        #######################################################\n",
    "        ######## PERFORM ICA ##################################\n",
    "        #######################################################\n",
    "\n",
    "        # perform ica and plot\n",
    "        # ica = mne.preprocessing.ICA(n_components=0.95, random_state=97, max_iter='auto')\n",
    "        # ica.fit(raw)\n",
    "        # ica_raw = ica.get_components()\n",
    "\n",
    "        #######################################################\n",
    "        ######## LOAD ANNOTATIONS FROM THE TSV FILE ###########\n",
    "        #######################################################\n",
    "\n",
    "        df = get_emotion_onset(tsv, duration = 100.0)\n",
    "\n",
    "        # add the annotations to the raw data\n",
    "        annot = mne.Annotations(\n",
    "            onset=list(df['onset']),  # in seconds\n",
    "            duration=list(np.array([60.0]*len(df['onset']))),  # in seconds, too\n",
    "            description=list(df['value']),\n",
    "        )\n",
    "\n",
    "        # print the annotations\n",
    "        # print(annot)\n",
    "\n",
    "        raw2 = raw.copy().set_annotations(annot)\n",
    "\n",
    "        # get events from the annotations\n",
    "        events_from_annot, event_dict = mne.events_from_annotations(raw2)\n",
    "\n",
    "        # epochs # 10 seconds before the onset and 90 seconds after the onset\n",
    "        epochs = mne.Epochs(raw2, events_from_annot, event_id=event_dict, tmin=-10, tmax=90, preload=True)\n",
    "\n",
    "        # apply baseline correction\n",
    "        epochs.apply_baseline((None, 0))\n",
    "\n",
    "        # filter the data\n",
    "        epochs.filter(1, 40)\n",
    "\n",
    "        # save the data\n",
    "        print('SAVED: '+base_path + 'data_{}.npz'.format(eeglab_file.split('/')[2]))\n",
    "        np.savez_compressed(base_path + 'data_{}.npz'.format(eeglab_file.split('/')[2]), data=np.array(epochs.get_data(),dtype='float32'))\n",
    "\n",
    "        # save dataframe\n",
    "        df.to_csv(base_path + 'data_{}.csv'.format(eeglab_file.split('/')[2]), index=False)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "base_path = './data/emotion_recognition_preprocessed_data/'\n",
    "\n",
    "get_data_eeglab(files_name_eeglab, files_name_tsv, base_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
