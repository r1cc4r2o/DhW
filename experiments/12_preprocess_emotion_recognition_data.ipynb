{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/emotion_recognition_preprocessed_data/data_sub-01.npz\n",
      "(14, 224, 25601)\n",
      "(14, 235, 25601)\n",
      "336909160\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-02.npz\n",
      "(15, 214, 25601)\n",
      "(15, 235, 25601)\n",
      "697883260\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-03.npz\n",
      "(15, 134, 25601)\n",
      "(15, 235, 25601)\n",
      "1058857360\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-04.npz\n",
      "(15, 222, 25601)\n",
      "(15, 235, 25601)\n",
      "1419831460\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-05.npz\n",
      "(15, 219, 25601)\n",
      "(15, 235, 25601)\n",
      "1780805560\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-06.npz\n",
      "(15, 221, 25601)\n",
      "(15, 235, 25601)\n",
      "2141779660\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-07.npz\n",
      "(15, 235, 25601)\n",
      "(15, 235, 25601)\n",
      "2502753760\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-08.npz\n",
      "(15, 221, 25601)\n",
      "(15, 235, 25601)\n",
      "2863727860\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-09.npz\n",
      "(15, 227, 25601)\n",
      "(15, 235, 25601)\n",
      "3224701960\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-10.npz\n",
      "(15, 209, 25601)\n",
      "(15, 235, 25601)\n",
      "3585676060\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-11.npz\n",
      "(15, 231, 25601)\n",
      "(15, 235, 25601)\n",
      "3946650160\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-12.npz\n",
      "(15, 220, 25601)\n",
      "(15, 235, 25601)\n",
      "4307624260\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-13.npz\n",
      "(15, 215, 25601)\n",
      "(15, 235, 25601)\n",
      "4668598360\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-14.npz\n",
      "(15, 219, 25601)\n",
      "(15, 235, 25601)\n",
      "5029572460\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-15.npz\n",
      "(14, 229, 25601)\n",
      "(14, 235, 25601)\n",
      "5366481620\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-16.npz\n",
      "(14, 212, 25601)\n",
      "(14, 235, 25601)\n",
      "5703390780\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-17.npz\n",
      "(15, 212, 25601)\n",
      "(15, 235, 25601)\n",
      "6064364880\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-18.npz\n",
      "(15, 224, 25601)\n",
      "(15, 235, 25601)\n",
      "6425338980\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-19.npz\n",
      "(15, 224, 25601)\n",
      "(15, 235, 25601)\n",
      "6786313080\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-20.npz\n",
      "(15, 218, 25601)\n",
      "(15, 235, 25601)\n",
      "7147287180\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-21.npz\n",
      "(15, 214, 25601)\n",
      "(15, 235, 25601)\n",
      "7508261280\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-23.npz\n",
      "(15, 206, 25601)\n",
      "(15, 235, 25601)\n",
      "7869235380\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-25.npz\n",
      "(15, 207, 25601)\n",
      "(15, 235, 25601)\n",
      "8230209480\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-26.npz\n",
      "(15, 211, 25601)\n",
      "(15, 235, 25601)\n",
      "8591183580\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-28.npz\n",
      "(15, 219, 25601)\n",
      "(15, 235, 25601)\n",
      "8952157680\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-29.npz\n",
      "(15, 232, 25601)\n",
      "(15, 235, 25601)\n",
      "9313131780\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-30.npz\n",
      "(15, 201, 25601)\n",
      "(15, 235, 25601)\n",
      "9674105880\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-31.npz\n",
      "(15, 213, 25601)\n",
      "(15, 235, 25601)\n",
      "10035079980\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-32.npz\n",
      "(15, 223, 25601)\n",
      "(15, 235, 25601)\n",
      "10396054080\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-34.npz\n",
      "(15, 180, 25601)\n",
      "(15, 235, 25601)\n",
      "10757028180\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-35.npz\n",
      "(15, 208, 25601)\n",
      "(15, 235, 25601)\n",
      "11118002280\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_name = sorted(glob.glob('./data/emotion*/*.npz'))\n",
    "byte = 0\n",
    "for file in file_name:\n",
    "    print(file)\n",
    "    data = np.load(file)\n",
    "    print(data['data'].shape)\n",
    "    # pad the secodn dimension\n",
    "    data = np.pad(data['data'], ((0, 0), (0, 235 - data['data'].shape[1]), (0, 0)), 'constant', constant_values=0)\n",
    "    print(data.shape)\n",
    "    # dimension in bytes\n",
    "    byte = byte + data.nbytes\n",
    "    print(byte)\n",
    "\n",
    "# full data size 11.3 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/emotion_recognition_preprocessed_data/data_sub-01.npz\n",
      "(14, 224, 12800)\n",
      "(14, 235, 12800)\n",
      "168448000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-02.npz\n",
      "(15, 214, 12800)\n",
      "(15, 235, 12800)\n",
      "348928000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-03.npz\n",
      "(15, 134, 12800)\n",
      "(15, 235, 12800)\n",
      "529408000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-04.npz\n",
      "(15, 222, 12800)\n",
      "(15, 235, 12800)\n",
      "709888000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-05.npz\n",
      "(15, 219, 12800)\n",
      "(15, 235, 12800)\n",
      "890368000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-06.npz\n",
      "(15, 221, 12800)\n",
      "(15, 235, 12800)\n",
      "1070848000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-07.npz\n",
      "(15, 235, 12800)\n",
      "(15, 235, 12800)\n",
      "1251328000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-08.npz\n",
      "(15, 221, 12800)\n",
      "(15, 235, 12800)\n",
      "1431808000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-09.npz\n",
      "(15, 227, 12800)\n",
      "(15, 235, 12800)\n",
      "1612288000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-10.npz\n",
      "(15, 209, 12800)\n",
      "(15, 235, 12800)\n",
      "1792768000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-11.npz\n",
      "(15, 231, 12800)\n",
      "(15, 235, 12800)\n",
      "1973248000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-12.npz\n",
      "(15, 220, 12800)\n",
      "(15, 235, 12800)\n",
      "2153728000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-13.npz\n",
      "(15, 215, 12800)\n",
      "(15, 235, 12800)\n",
      "2334208000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-14.npz\n",
      "(15, 219, 12800)\n",
      "(15, 235, 12800)\n",
      "2514688000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-15.npz\n",
      "(14, 229, 12800)\n",
      "(14, 235, 12800)\n",
      "2683136000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-16.npz\n",
      "(14, 212, 12800)\n",
      "(14, 235, 12800)\n",
      "2851584000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-17.npz\n",
      "(15, 212, 12800)\n",
      "(15, 235, 12800)\n",
      "3032064000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-18.npz\n",
      "(15, 224, 12800)\n",
      "(15, 235, 12800)\n",
      "3212544000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-19.npz\n",
      "(15, 224, 12800)\n",
      "(15, 235, 12800)\n",
      "3393024000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-20.npz\n",
      "(15, 218, 12800)\n",
      "(15, 235, 12800)\n",
      "3573504000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-21.npz\n",
      "(15, 214, 12800)\n",
      "(15, 235, 12800)\n",
      "3753984000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-23.npz\n",
      "(15, 206, 12800)\n",
      "(15, 235, 12800)\n",
      "3934464000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-25.npz\n",
      "(15, 207, 12800)\n",
      "(15, 235, 12800)\n",
      "4114944000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-26.npz\n",
      "(15, 211, 12800)\n",
      "(15, 235, 12800)\n",
      "4295424000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-28.npz\n",
      "(15, 219, 12800)\n",
      "(15, 235, 12800)\n",
      "4475904000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-29.npz\n",
      "(15, 232, 12800)\n",
      "(15, 235, 12800)\n",
      "4656384000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-30.npz\n",
      "(15, 201, 12800)\n",
      "(15, 235, 12800)\n",
      "4836864000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-31.npz\n",
      "(15, 213, 12800)\n",
      "(15, 235, 12800)\n",
      "5017344000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-32.npz\n",
      "(15, 223, 12800)\n",
      "(15, 235, 12800)\n",
      "5197824000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-34.npz\n",
      "(15, 180, 12800)\n",
      "(15, 235, 12800)\n",
      "5378304000\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-35.npz\n",
      "(15, 208, 12800)\n",
      "(15, 235, 12800)\n",
      "5558784000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_name = sorted(glob.glob('./data/emotion*/*.npz'))\n",
    "byte = 0\n",
    "for file in file_name:\n",
    "    print(file)\n",
    "    data = np.load(file)\n",
    "    print(data['data'][:,:,:12800].shape)\n",
    "    # pad the secodn dimension\n",
    "    data = np.pad(data['data'][:,:,:12800], ((0, 0), (0, 235 - data['data'].shape[1]), (0, 0)), 'constant', constant_values=0)\n",
    "    print(data.shape)\n",
    "    # dimension in bytes\n",
    "    byte = byte + data.nbytes\n",
    "    print(byte)\n",
    "\n",
    "# full data size 5.55 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "'./data/emotion_recognition_preprocessed_data/preprocessed/'\n",
    "\n",
    "\n",
    "file_name = sorted(glob.glob('./data/emotion*/*.npz'))\n",
    "participants = pd.read_csv('./emotion_recognition_data/participants.tsv', sep='\\t').sort_values(by=['participant_id']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "byte = 0\n",
    "for file in file_name:\n",
    "    print(file)\n",
    "    data = np.load(file)\n",
    "    print(data['data'][:,:,:16800].shape)\n",
    "    # pad the secodn dimension\n",
    "    data = np.pad(data['data'][:,:,:16800], ((0, 0), (0, 235 - data['data'].shape[1]), (0, 0)), 'constant', constant_values=0)\n",
    "    print(data.shape)\n",
    "    # dimension in bytes\n",
    "    byte = byte + data.nbytes\n",
    "    print(byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Group</th>\n",
       "      <th>YOB</th>\n",
       "      <th>Handedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-01</td>\n",
       "      <td>F</td>\n",
       "      <td>23</td>\n",
       "      <td>normal</td>\n",
       "      <td>1981</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-02</td>\n",
       "      <td>M</td>\n",
       "      <td>21</td>\n",
       "      <td>normal</td>\n",
       "      <td>1983</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-03</td>\n",
       "      <td>F</td>\n",
       "      <td>22</td>\n",
       "      <td>normal</td>\n",
       "      <td>1982</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-04</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "      <td>normal</td>\n",
       "      <td>1975</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-05</td>\n",
       "      <td>F</td>\n",
       "      <td>26</td>\n",
       "      <td>normal</td>\n",
       "      <td>1978</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub-06</td>\n",
       "      <td>F</td>\n",
       "      <td>22</td>\n",
       "      <td>normal</td>\n",
       "      <td>1982</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub-07</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>normal</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub-08</td>\n",
       "      <td>F</td>\n",
       "      <td>23</td>\n",
       "      <td>normal</td>\n",
       "      <td>1981</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub-09</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>normal</td>\n",
       "      <td>1979</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub-10</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>normal</td>\n",
       "      <td>1975</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub-11</td>\n",
       "      <td>M</td>\n",
       "      <td>22</td>\n",
       "      <td>normal</td>\n",
       "      <td>1982</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sub-12</td>\n",
       "      <td>F</td>\n",
       "      <td>20</td>\n",
       "      <td>normal</td>\n",
       "      <td>1984</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sub-13</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>normal</td>\n",
       "      <td>1981</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sub-14</td>\n",
       "      <td>F</td>\n",
       "      <td>22</td>\n",
       "      <td>normal</td>\n",
       "      <td>1981</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sub-15</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>normal</td>\n",
       "      <td>1970</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub-16</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "      <td>normal</td>\n",
       "      <td>1975</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sub-17</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "      <td>normal</td>\n",
       "      <td>1972</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sub-18</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>normal</td>\n",
       "      <td>1974</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sub-19</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>normal</td>\n",
       "      <td>1986</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sub-20</td>\n",
       "      <td>M</td>\n",
       "      <td>33</td>\n",
       "      <td>normal</td>\n",
       "      <td>1971</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sub-21</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>normal</td>\n",
       "      <td>1985</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sub-23</td>\n",
       "      <td>F</td>\n",
       "      <td>22</td>\n",
       "      <td>normal</td>\n",
       "      <td>1982</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sub-24</td>\n",
       "      <td>F</td>\n",
       "      <td>38</td>\n",
       "      <td>normal</td>\n",
       "      <td>1966</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sub-25</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>normal</td>\n",
       "      <td>1979</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sub-26</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>normal</td>\n",
       "      <td>1976</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sub-27</td>\n",
       "      <td>F</td>\n",
       "      <td>26</td>\n",
       "      <td>normal</td>\n",
       "      <td>1978</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sub-28</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>normal</td>\n",
       "      <td>1975</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sub-29</td>\n",
       "      <td>F</td>\n",
       "      <td>21</td>\n",
       "      <td>normal</td>\n",
       "      <td>1983</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sub-30</td>\n",
       "      <td>M</td>\n",
       "      <td>22</td>\n",
       "      <td>normal</td>\n",
       "      <td>1982</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sub-31</td>\n",
       "      <td>M</td>\n",
       "      <td>22</td>\n",
       "      <td>normal</td>\n",
       "      <td>1982</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sub-32</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "      <td>normal</td>\n",
       "      <td>1976</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sub-33</td>\n",
       "      <td>F</td>\n",
       "      <td>21</td>\n",
       "      <td>normal</td>\n",
       "      <td>1983</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sub-34</td>\n",
       "      <td>M</td>\n",
       "      <td>21</td>\n",
       "      <td>normal</td>\n",
       "      <td>1983</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sub-35</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "      <td>normal</td>\n",
       "      <td>1977</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id Gender  Age   Group   YOB Handedness\n",
       "0          sub-01      F   23  normal  1981        NaN\n",
       "1          sub-02      M   21  normal  1983          R\n",
       "2          sub-03      F   22  normal  1982          R\n",
       "3          sub-04      F   29  normal  1975          R\n",
       "4          sub-05      F   26  normal  1978          R\n",
       "5          sub-06      F   22  normal  1982          R\n",
       "6          sub-07      F   24  normal  1980        NaN\n",
       "7          sub-08      F   23  normal  1981          R\n",
       "8          sub-09      F   25  normal  1979          R\n",
       "9          sub-10      M   29  normal  1975          R\n",
       "10         sub-11      M   22  normal  1982          R\n",
       "11         sub-12      F   20  normal  1984          R\n",
       "12         sub-13      M   23  normal  1981          R\n",
       "13         sub-14      F   22  normal  1981          R\n",
       "14         sub-15      M   35  normal  1970          R\n",
       "15         sub-16      F   29  normal  1975          R\n",
       "16         sub-17      F   32  normal  1972          L\n",
       "17         sub-18      M   30  normal  1974          R\n",
       "18         sub-19      F   18  normal  1986          R\n",
       "19         sub-20      M   33  normal  1971          R\n",
       "20         sub-21      F   19  normal  1985          R\n",
       "21         sub-23      F   22  normal  1982          R\n",
       "22         sub-24      F   38  normal  1966          R\n",
       "23         sub-25      M   25  normal  1979          R\n",
       "24         sub-26      F   28  normal  1976          R\n",
       "25         sub-27      F   26  normal  1978          R\n",
       "26         sub-28      M   29  normal  1975          R\n",
       "27         sub-29      F   21  normal  1983          R\n",
       "28         sub-30      M   22  normal  1982          R\n",
       "29         sub-31      M   22  normal  1982          R\n",
       "30         sub-32      M   28  normal  1976          R\n",
       "31         sub-33      F   21  normal  1983          R\n",
       "32         sub-34      M   21  normal  1983          R\n",
       "33         sub-35      M   28  normal  1977          L"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# sort by participant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/emotion_recognition_preprocessed_data/data_sub-01.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-02.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-03.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-04.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-05.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-06.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-07.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-08.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-09.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-10.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-11.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-12.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-13.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-14.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-15.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-16.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-17.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-18.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-19.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-20.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-21.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-23.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-25.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-26.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-28.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-29.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-30.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-31.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-32.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-34.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-35.npz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.tensor(data).dtype\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "gender_map = {'M': 0, 'F': 1}\n",
    "age_map = {'25-35': 0, '35-45': 1, '45-55': 2, '55+': 3}\n",
    "part_id = {\n",
    "    'sub-01': 0,\n",
    "    'sub-02': 1,\n",
    "    'sub-03': 2,\n",
    "    'sub-04': 3,\n",
    "    'sub-05': 4,\n",
    "    'sub-06': 5,\n",
    "    'sub-07': 6,\n",
    "    'sub-08': 7,\n",
    "    'sub-09': 8,\n",
    "    'sub-10': 9,\n",
    "    'sub-11': 10,\n",
    "    'sub-12': 11,\n",
    "    'sub-13': 12,\n",
    "    'sub-14': 13,\n",
    "    'sub-15': 14,\n",
    "    'sub-16': 15,\n",
    "    'sub-17': 16,\n",
    "    'sub-18': 17,\n",
    "    'sub-19': 18,\n",
    "    'sub-20': 19,\n",
    "    'sub-21': 20,\n",
    "    'sub-22': 21,\n",
    "    'sub-23': 22,\n",
    "    'sub-24': 23,\n",
    "    'sub-25': 24,\n",
    "    'sub-26': 25,\n",
    "    'sub-27': 26,\n",
    "    'sub-28': 27,\n",
    "    'sub-29': 28,\n",
    "    'sub-30': 29,\n",
    "    'sub-31': 30,\n",
    "    'sub-32': 31,\n",
    "    'sub-33': 32,\n",
    "    'sub-34': 33,\n",
    "    'sub-35': 34\n",
    "}\n",
    "\n",
    "emotions = {'awe': 0,\n",
    "            'fear': 1,\n",
    "            'joy': 2,\n",
    "            'jealousy': 3,\n",
    "            'excite': 4,\n",
    "            'disgust': 5,\n",
    "            'love': 6,\n",
    "            'grief': 7,\n",
    "            'content': 8,\n",
    "            'frustration': 9,\n",
    "            'happy': 10,\n",
    "            'anger': 11,\n",
    "            'relief': 12,\n",
    "            'sad': 13,\n",
    "            'compassion': 14}\n",
    "\n",
    "\n",
    "info = torch.tensor([])\n",
    "data_X = torch.tensor([])\n",
    "data_y = torch.tensor([])\n",
    "\n",
    "number_steps = 15\n",
    "length_s = 16800\n",
    "shift = 1000000000\n",
    "\n",
    "ind = np.linspace(0, length_s, number_steps, dtype=int)\n",
    "file_name = sorted(glob.glob('./data/emotion*/*.npz'))\n",
    "participants = pd.read_csv('./emotion_recognition_data/participants.tsv', sep='\\t').sort_values(by=['participant_id']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "byte = 0\n",
    "# for file in file_name:\n",
    "for file in file_name:\n",
    "    print(file)\n",
    "    data = np.load(file)\n",
    "    data = np.pad(data['data'][:,:,:length_s] * shift, ((0, 0), (0, 235 - data['data'].shape[1]), (0, 0)), 'constant', constant_values=0)\n",
    "\n",
    "    data_cutted = torch.cat([torch.stack([torch.tensor(d[:,m:M], dtype=torch.float32) for m, M in zip(ind[:-1], ind[1:])]) for d in data])\n",
    "\n",
    "    participant_info = participants[participants['participant_id'] == file.split('/')[-1].split('_')[1].split('.')[0]].values[0]\n",
    "\n",
    "    participant_info = torch.tensor(\n",
    "                            np.array([np.array([part_id[participant_info[0]], gender_map[participant_info[1]], participant_info[2], participant_info[4]])]*data_cutted.shape[0]), \n",
    "                                dtype=torch.int16)\n",
    "\n",
    "    subj_id = file.split('/')[-1].split('_')[1].split('.')[0]\n",
    "    l = torch.tensor([emotions[emotion] for emotion in pd.read_csv(f'./data/emotion_recognition_preprocessed_data/data_{subj_id}.csv')['value'].values]*(number_steps-1))\n",
    "\n",
    "    # save info in list\n",
    "    info = torch.cat([info, participant_info])\n",
    "    data_X = torch.cat([data_X, data_cutted])\n",
    "    data_y = torch.cat([data_y, l])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del\n",
    "del data_cutted, participant_info, subj_id, l, data\n",
    "\n",
    "# save data\n",
    "torch.save(data_X, './data/emotion_recognition_preprocessed_data/preprocessed_cutted/data.pt')\n",
    "\n",
    "del data_X\n",
    "\n",
    "torch.save(info, './data/emotion_recognition_preprocessed_data/preprocessed_cutted/info.pt')\n",
    "torch.save(data_y, './data/emotion_recognition_preprocessed_data/preprocessed_cutted/labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/emotion_recognition_preprocessed_data/data_sub-01.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-02.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-03.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-04.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-05.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-06.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-07.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-08.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-09.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-10.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-11.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-12.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-13.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-14.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-15.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-16.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-17.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-18.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-19.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-20.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-21.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-23.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-25.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-26.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-28.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-29.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-30.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-31.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-32.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-34.npz\n",
      "./data/emotion_recognition_preprocessed_data/data_sub-35.npz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.tensor(data).dtype\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "gender_map = {'M': 0, 'F': 1}\n",
    "age_map = {'25-35': 0, '35-45': 1, '45-55': 2, '55+': 3}\n",
    "part_id = {\n",
    "    'sub-01': 0,\n",
    "    'sub-02': 1,\n",
    "    'sub-03': 2,\n",
    "    'sub-04': 3,\n",
    "    'sub-05': 4,\n",
    "    'sub-06': 5,\n",
    "    'sub-07': 6,\n",
    "    'sub-08': 7,\n",
    "    'sub-09': 8,\n",
    "    'sub-10': 9,\n",
    "    'sub-11': 10,\n",
    "    'sub-12': 11,\n",
    "    'sub-13': 12,\n",
    "    'sub-14': 13,\n",
    "    'sub-15': 14,\n",
    "    'sub-16': 15,\n",
    "    'sub-17': 16,\n",
    "    'sub-18': 17,\n",
    "    'sub-19': 18,\n",
    "    'sub-20': 19,\n",
    "    'sub-21': 20,\n",
    "    'sub-22': 21,\n",
    "    'sub-23': 22,\n",
    "    'sub-24': 23,\n",
    "    'sub-25': 24,\n",
    "    'sub-26': 25,\n",
    "    'sub-27': 26,\n",
    "    'sub-28': 27,\n",
    "    'sub-29': 28,\n",
    "    'sub-30': 29,\n",
    "    'sub-31': 30,\n",
    "    'sub-32': 31,\n",
    "    'sub-33': 32,\n",
    "    'sub-34': 33,\n",
    "    'sub-35': 34\n",
    "}\n",
    "\n",
    "emotions = {'awe': 0,\n",
    "            'fear': 1,\n",
    "            'joy': 2,\n",
    "            'jealousy': 3,\n",
    "            'excite': 4,\n",
    "            'disgust': 5,\n",
    "            'love': 6,\n",
    "            'grief': 7,\n",
    "            'content': 8,\n",
    "            'frustration': 9,\n",
    "            'happy': 10,\n",
    "            'anger': 11,\n",
    "            'relief': 12,\n",
    "            'sad': 13,\n",
    "            'compassion': 14}\n",
    "\n",
    "\n",
    "info = torch.tensor([])\n",
    "data_X = torch.tensor([])\n",
    "data_y = torch.tensor([])\n",
    "\n",
    "number_steps = 15\n",
    "length_s = 16800\n",
    "shift = 1000000000\n",
    "\n",
    "ind = np.linspace(0, length_s, number_steps, dtype=int)\n",
    "file_name = sorted(glob.glob('./data/emotion*/*.npz'))\n",
    "participants = pd.read_csv('./emotion_recognition_data/participants.tsv', sep='\\t').sort_values(by=['participant_id']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "byte = 0\n",
    "# for file in file_name:\n",
    "for file in file_name:\n",
    "    print(file)\n",
    "    data = np.load(file)\n",
    "    # add shift\n",
    "    data = np.pad(data['data'][:,:,:length_s] * shift, ((0, 0), (0, 235 - data['data'].shape[1]), (0, 0)), 'constant', constant_values=0)\n",
    "\n",
    "    data_cutted = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    participant_info = participants[participants['participant_id'] == file.split('/')[-1].split('_')[1].split('.')[0]].values[0]\n",
    "\n",
    "    participant_info = torch.tensor(\n",
    "                            np.array([np.array([part_id[participant_info[0]], gender_map[participant_info[1]], participant_info[2], participant_info[4]])]*data_cutted.shape[0]), \n",
    "                                dtype=torch.int16)\n",
    "\n",
    "    subj_id = file.split('/')[-1].split('_')[1].split('.')[0]\n",
    "    l = torch.tensor([emotions[emotion] for emotion in pd.read_csv(f'./data/emotion_recognition_preprocessed_data/data_{subj_id}.csv')['value'].values], dtype=torch.int16)\n",
    "\n",
    "    # save info in list\n",
    "    info = torch.cat([info, participant_info])\n",
    "    data_X = torch.cat([data_X, data_cutted])\n",
    "    data_y = torch.cat([data_y, l])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, data_cutted, participant_info, subj_id, l\n",
    "# save tensor\n",
    "\n",
    "torch.save(data_X, './data/emotion_recognition_preprocessed_data/preprocessed/data.pt')\n",
    "\n",
    "del data_X\n",
    "torch.save(info, './data/emotion_recognition_preprocessed_data/preprocessed/info.pt')\n",
    "torch.save(data_y, './data/emotion_recognition_preprocessed_data/preprocessed/labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6468, 235, 1200])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load('./data/emotion_recognition_preprocessed_data/preprocessed/data.pt')\n",
    "info = torch.load('./data/emotion_recognition_preprocessed_data/preprocessed/info.pt')\n",
    "labels = torch.load('./data/emotion_recognition_preprocessed_data/preprocessed/labels.pt')\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "BACH_SIZE = 64\n",
    "\n",
    "dataset = TensorDataset(data, info, labels)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BACH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BACH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BACH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4504686e-09"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(file)['data'].mean()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uri_mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
