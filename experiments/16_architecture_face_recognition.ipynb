{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "# iter_freqs = [(\"Theta\", 4, 7), (\"Alpha\", 8, 12), (\"Beta\", 13, 25), (\"Gamma\", 30, 45)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.colab.drive\n",
    "google.colab.drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout(p=0.1, inplace=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open(\"./data/face_recognition_preprocessed_data/dataset/event_dict_all.pkl\", \"rb\") as f:\n",
    "    event_dict_all = pickle.load(f)\n",
    "\n",
    "print(event_dict_all[:10])\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/dataset/target_trial_type.npy'\n",
    "\n",
    "target_trial_type = np.load(path)\n",
    "\n",
    "print(target_trial_type.shape)\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/dataset/target_labels.pt'\n",
    "\n",
    "target_labels = torch.load(path)\n",
    "\n",
    "print(target_labels.shape)\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/dataset/X.pt'\n",
    "\n",
    "X = torch.load(path)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing for face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "dict_label = {\"5\": \"Initial presentation of famous face\",\n",
    "                \"6\": \"Immediate repeated  presentation of famous face\",\n",
    "                \"7\": \"Delayed repeated  presentation of famous face\",\n",
    "                \"13\": \"Initial presentation of unfamiliar face\",\n",
    "                \"14\": \"Immediate repeated  presentation of unfamiliar face\",\n",
    "                \"15\": \"Delayed repeated  presentation of unfamiliar face\",\n",
    "                \"17\": \"Initial presentation of scrambled face\",\n",
    "                \"18\": \"Immediate repeated  presentation of scrambled face\",\n",
    "                \"19\": \"Delayed repeated  presentation of scrambled face\",\n",
    "                \"256\": \"Left button press\",\n",
    "                \"4096\": \"Right button press\"}\n",
    "\n",
    "print(dict_label)\n",
    "\n",
    "classes = [5.0, 6.0, 7.0, 13.0, 14.0, 15.0, 17.0, 18.0, 19.0]\n",
    "pick_number_of_samples_per_class = [2600, 1200, 1200, 2500, 1200, 1200, 2600, 1200, 1200]\n",
    "number_of_samples_per_class = [2628, 1273, 1253, 2588, 1252, 1257, 2614, 1234, 1293]\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/dataset/target_labels.pt'\n",
    "\n",
    "labels_face = torch.load(path)\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/dataset/X.pt'\n",
    "\n",
    "y_signal = torch.load(path)\n",
    "\n",
    "print(y_signal.shape)\n",
    "\n",
    "print(labels_face.shape)\n",
    "\n",
    "path = glob.glob('./data/face_recognition_preprocessed_data/tfr_dataset/data_per_class_*.pt')\n",
    "\n",
    "lenght = sum([torch.load(p).shape[0] for p in path])\n",
    "print('Number of frequency maps ',lenght)\n",
    "\n",
    "\n",
    "# retrieve the respective samples and labels \n",
    "# these will be used to train the model in a multi-task fashion\n",
    "#       the model will be trained to regress y_signal (the signal) from the frequency maps\n",
    "#       whereas the dict_label are the classes that we try to predict\n",
    "\n",
    "#       additionally, I use as class tocken (CLS) the labels_face (describe the type of face that was presented)\n",
    "#       the tocken condition the transformer given the feature extracted from the frequency maps\n",
    "\n",
    "\n",
    "conta = 0\n",
    "y_sample = []\n",
    "y_label = []\n",
    "for n_sample_class, sample_to_pick in zip(number_of_samples_per_class, pick_number_of_samples_per_class):\n",
    "    print()\n",
    "    print('Pick the respective samples:',y_signal[conta:conta+sample_to_pick].shape)\n",
    "    print('Pick the respective labels:',labels_face[conta:conta+sample_to_pick].shape)\n",
    "    y_sample.append(y_signal[conta:conta+sample_to_pick])\n",
    "    y_label.append(labels_face[conta:conta+sample_to_pick])\n",
    "    conta += n_sample_class\n",
    "\n",
    "\n",
    "y_sample = torch.cat(y_sample, dim=0)\n",
    "y_label = torch.cat(y_label, dim=0)\n",
    "\n",
    "# # save the data\n",
    "# path = './data/face_recognition_preprocessed_data/tfr_dataset/y_sample.pt'\n",
    "# torch.save(y_sample, path)\n",
    "# path = './data/face_recognition_preprocessed_data/tfr_dataset/y_label.pt'\n",
    "# torch.save(y_label, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample.shape, y_label.shape, torch.load('./data/face_recognition_preprocessed_data/tfr_dataset/data_per_class_5.0.pt').shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture for face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from typing import Any\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Apply a two steps convolution with normalization and GELU activation\"\"\"\n",
    "    def __init__(self, channel_in, channel_out, shape_norm, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.ln = nn.LayerNorm(shape_norm)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "class UnetEncoderBlock(nn.Module):\n",
    "    \"\"\"Downsaple the input by a factor 2 raising the number of channels by a factor 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        shape_norm = [out_channels, 26, 189]\n",
    "        self.convblock = ConvBlock(in_channels, out_channels, shape_norm)\n",
    "        self.maxpool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convblock(x)\n",
    "        # print(x.shape)\n",
    "        pooled = self.maxpool(x)\n",
    "        # print(pooled.shape)\n",
    "        return x, pooled\n",
    "\n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "class UnetDecoderBlock(nn.Module):\n",
    "    \"\"\"Upsample the input by a factor 2 lowering the number of channels by a factor 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # self.conv1 = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,  dilation=1, kernel_size=3, padding=0, stride=2)#dilation=1,\n",
    "        # self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,  dilation=1, kernel_size=3, padding=1)#dilation=1,\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,  kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        shape_norm = [out_channels,  26, 189]\n",
    "        self.convblock = ConvBlock(in_channels+out_channels, out_channels, shape_norm)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        skip = self.conv1(skip)\n",
    "        # print(x.shape, skip.shape)\n",
    "        # pad x to match skip   \n",
    "        # torch.Size([32, 74, 26, 188]) torch.Size([32, 148, 26, 189])\n",
    "        skip = nn.functional.pad(skip, (0, 1, 0, 0))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"Unet architecture\"\"\"\n",
    "    def __init__(self, encoder_steps = 2, decoder_steps = 2, n_channels = 74):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_steps = encoder_steps\n",
    "        self.decoder_steps = decoder_steps\n",
    "\n",
    "\n",
    "        # encoder\n",
    "        self.encoder_1 = UnetEncoderBlock(n_channels, n_channels*2)\n",
    "        self.encoder_2 = UnetEncoderBlock(n_channels*2, n_channels*4)\n",
    "        # decoder\n",
    "        self.decoder_1 = UnetDecoderBlock(n_channels*4, n_channels*2)\n",
    "        self.decoder_2 = UnetDecoderBlock(n_channels*2, n_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder\n",
    "        x, pooled_1 = self.encoder_1(x)\n",
    "        x, pooled_2 = self.encoder_2(x) # bottleneck\n",
    "\n",
    "        # decoder\n",
    "        x = self.decoder_1(x, pooled_2)\n",
    "        x = self.decoder_2(x, pooled_1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, n_channels = 74, kernel_size=3, stride=2, padding=0, dilation=1, dropout=0.1, stride_factor_reduction = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.channel_reduction = 16\n",
    "        self.stride_factor_reduction = stride_factor_reduction\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels//self.channel_reduction, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels//self.channel_reduction, out_channels=n_channels//self.channel_reduction, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm([n_channels//self.channel_reduction, 26//self.stride_factor_reduction-1, 189//self.stride_factor_reduction])\n",
    "        self.layer_norm_2 = nn.LayerNorm([n_channels//self.channel_reduction, (26//self.stride_factor_reduction-1)//self.stride_factor_reduction-1, (189//self.stride_factor_reduction -1)//self.stride_factor_reduction])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # feature extraction block 1\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # feature extraction block 2\n",
    "        x = self.conv2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.flatten(start_dim=2, end_dim=-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class FeatureTransformerBlock(nn.Module):   \n",
    "    \"\"\"Transformer block for the feature extraction\"\"\"\n",
    "    def __init__(self, lenght_signal = 189, num_t_layer = 1, n_head = 3, stride_factor_reduction = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_t_layer = num_t_layer\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        self.stride_factor_reduction = stride_factor_reduction\n",
    "\n",
    "        freq = (26//self.stride_factor_reduction-1)//self.stride_factor_reduction-1\n",
    "        time_dim = (189//self.stride_factor_reduction -1)//self.stride_factor_reduction\n",
    "\n",
    "        self.linear_transformation = nn.Linear(freq*time_dim, lenght_signal, bias=True)\n",
    "\n",
    "        # 768 is the dimension of the embedding of BERT\n",
    "        self.linear_transformation_tface = nn.Linear(768, lenght_signal, bias=True)\n",
    "        \n",
    "        self.te_layer = nn.TransformerEncoderLayer(d_model=lenght_signal, nhead=n_head)\n",
    "        self.te = nn.TransformerEncoder(self.te_layer, num_layers=num_t_layer)\n",
    "\n",
    "    def forward(self, x, t_face):\n",
    "        \"\"\"Return the cls token and the embedding\"\"\"\n",
    "\n",
    "        # linear transformation\n",
    "        x = self.linear_transformation(x)\n",
    "\n",
    "        # add cls token\n",
    "        # cls_token = t_face.unsqueeze(-1).to(x.device)\n",
    "        cls_token = t_face.to(x.device)\n",
    "        cls_token = self.linear_transformation_tface(cls_token).unsqueeze(-2)\n",
    "        # print(cls_token.shape, x.shape)\n",
    "        # cls_token = torch.zeros(x.shape[0], 1, x.shape[2]).to(x.device)\n",
    "        \n",
    "        # concat cls token with the embedding\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "\n",
    "        # transpose to match transformer input\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # transformer\n",
    "        x = self.te(x)\n",
    "\n",
    "        # transpose back\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # get the cls token\n",
    "        cls_token = x[:, 0, :]\n",
    "\n",
    "        # get the embedding\n",
    "        x = x[:, 1:, :].permute(0, 2, 1)\n",
    "\n",
    "        return cls_token, x\n",
    "    \n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "class Architecture(nn.Module):\n",
    "    \"\"\"Net architecture\"\"\"\n",
    "    def __init__(self, n_channels = 74, lenght_signal = 189, n_classes = 9):\n",
    "        super().__init__()\n",
    "\n",
    "        self.unet = Unet()\n",
    "        self.feature_extractor = FeatureExtractor(n_channels = 74)\n",
    "        self.feature_transformer = FeatureTransformerBlock()\n",
    "\n",
    "        self.up_sampling_channels = nn.Linear(4, n_channels)\n",
    "        self.mlp_classifier = nn.Sequential(\n",
    "            nn.Linear(lenght_signal, lenght_signal//8),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(lenght_signal//8),\n",
    "            nn.Linear(lenght_signal//8, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_face):\n",
    "        # unet\n",
    "        x = self.unet(x)\n",
    "        # feature extractor\n",
    "        x = self.feature_extractor(x)\n",
    "        # feature transformer\n",
    "        cls_token, x = self.feature_transformer(x, t_face)\n",
    "\n",
    "        # up sampling\n",
    "        x = self.up_sampling_channels(x).permute(0, 2, 1)\n",
    "\n",
    "        # mlp classifier\n",
    "        cls_classification = self.mlp_classifier(cls_token)\n",
    "\n",
    "        return cls_classification, x\n",
    "    \n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self, n_channels = 74, lenght_signal = 189, n_classes = 9):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = Architecture(n_channels = n_channels, lenght_signal = lenght_signal, n_classes = n_classes)\n",
    "\n",
    "        self.criterion_classification = nn.CrossEntropyLoss()\n",
    "        self.criterion_regression = nn.MSELoss()\n",
    "        self.huber_loss = nn.SmoothL1Loss()\n",
    "        self.classification_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=n_classes)\n",
    "\n",
    "    def forward(self, x, t_face):\n",
    "        return self.model(x, t_face)\n",
    "    \n",
    "    def training_step(self, x, batch_idx):\n",
    "        # get the data\n",
    "        x, x_face, y_label_signal, y_label_class = x\n",
    "\n",
    "        cls_classification, x = self.forward(x, x_face)\n",
    "\n",
    "        # classification loss\n",
    "        loss_classification = self.criterion_classification(cls_classification, y_label_class)\n",
    "\n",
    "        # regression loss\n",
    "        loss_regression = self.criterion_regression(x, y_label_signal)\n",
    "\n",
    "        # total loss\n",
    "        loss = loss_classification + loss_regression\n",
    "\n",
    "        huber_loss = self.huber_loss(x, y_label_signal)\n",
    "\n",
    "        y_label_class = torch.argmax(y_label_class, dim = -1)\n",
    "\n",
    "        # classification accuracy\n",
    "        acc = self.classification_accuracy(cls_classification, y_label_class)\n",
    "\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_loss_classification', loss_classification, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_loss_regression', loss_regression, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_huber_loss', huber_loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc_classification', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, x, batch_idx):\n",
    "        # get the data\n",
    "        x, x_face, y_label_signal, y_label_class = x\n",
    "\n",
    "        cls_classification, x = self.forward(x, x_face)\n",
    "\n",
    "        # classification loss\n",
    "        loss_classification = self.criterion_classification(cls_classification, y_label_class)\n",
    "\n",
    "        # regression loss\n",
    "        loss_regression = self.criterion_regression(x, y_label_signal)\n",
    "\n",
    "        # total loss\n",
    "        loss = loss_classification + loss_regression\n",
    "\n",
    "        huber_loss = self.huber_loss(x, y_label_signal)\n",
    "\n",
    "        y_label_class = torch.argmax(y_label_class, dim = -1)\n",
    "\n",
    "        # classification accuracy\n",
    "        acc = self.classification_accuracy(cls_classification, y_label_class)\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_loss_classification', loss_classification, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_loss_regression', loss_regression, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_huber_loss', huber_loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc_classification', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for face recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare the data to be used for face recognition. Due to the memory load I decided to reduce the number of samples to 700 for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14900]), 15392, torch.Size([14900, 74, 189]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_face_full.shape, sum(number_of_samples_per_class), y_full.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to pick class and event id from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_05.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_06.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_07.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_13.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_14.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_15.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_17.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_18.pt', './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_19.pt']\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_05.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_06.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_07.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_13.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_14.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_15.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_17.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_18.pt\n",
      "./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_19.pt\n",
      "torch.Size([9000, 74, 26, 189]) torch.Size([9000]) torch.Size([9000, 74, 189]) torch.Size([9000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "dict_classes = {5: 0, 6: 1, 7: 2, 13: 3, 14: 4, 15: 5, 17: 6, 18: 7, 19: 8}\n",
    "classes = [5.0, 6.0, 7.0, 13.0, 14.0, 15.0, 17.0, 18.0, 19.0]\n",
    "number_of_samples_per_class = [2628, 1273, 1253, 2588, 1252, 1257, 2614, 1234, 1293]\n",
    "pick_number_of_samples_per_class = [2600, 1200, 1200, 2500, 1200, 1200, 2600, 1200, 1200]\n",
    "\n",
    "# signal to regrss\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/y_sample.pt'\n",
    "y_full = torch.load(path)\n",
    "\n",
    "# face cls\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/y_label.pt'\n",
    "x_face_full = torch.load(path)\n",
    "\n",
    "path = sorted(glob.glob('./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_*.pt'))\n",
    "\n",
    "print(path)\n",
    "\n",
    "conta = 0\n",
    "number_sample_per_class = 1000\n",
    "\n",
    "X = []\n",
    "x_face = []\n",
    "y_label_signal = []\n",
    "y_label_class = []\n",
    "\n",
    "for p, n_sample_class in zip(path, pick_number_of_samples_per_class):\n",
    "    print(p)\n",
    "    data = torch.load(p)\n",
    "    data = data[:number_sample_per_class].type(torch.float16)\n",
    "\n",
    "    # feature maps with the presence of the frequencies\n",
    "    X.append(data)\n",
    "\n",
    "    # face cls\n",
    "    x_face.append(x_face_full[conta:conta+number_sample_per_class])\n",
    "\n",
    "    # signal to regress\n",
    "    y_label_signal.append(y_full[conta:conta+number_sample_per_class]*1e3)\n",
    "\n",
    "    # class\n",
    "    class_id = torch.ones(number_sample_per_class, dtype=torch.int8)*int(p.split('_')[-1].split('.')[0])\n",
    "    y_label_class.append(class_id)\n",
    "\n",
    "    conta = conta + n_sample_class\n",
    "\n",
    "X = torch.cat(X, dim = 0)\n",
    "x_face = torch.cat(x_face, dim = 0)\n",
    "y_label_signal = torch.cat(y_label_signal, dim = 0)\n",
    "y_label_class = torch.cat(y_label_class, dim = 0)\n",
    "\n",
    "print(X.shape, x_face.shape, y_label_signal.shape, y_label_class.shape)\n",
    "# torch.Size([9000, 74, 26, 189]) torch.Size([9000]) torch.Size([9000, 74, 189]) torch.Size([9000])\n",
    "\n",
    "# torch.save(X, './data/face_recognition_preprocessed_data/tfr_dataset/X.pt')\n",
    "# torch.save(x_face, './data/face_recognition_preprocessed_data/tfr_dataset/x_face.pt')\n",
    "# torch.save(y_label_signal, './data/face_recognition_preprocessed_data/tfr_dataset/y_label_signal.pt')\n",
    "# torch.save(y_label_class, './data/face_recognition_preprocessed_data/tfr_dataset/y_label_class.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of event embedd the description that will become the cls component to condition the transformer. Now for each face I encode the type of face in the latent space of a pretrained LLM (BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face cls\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# load pre-trained LLM from huggerface\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "# freeze all the parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def get_features(model, x):\n",
    "    with torch.no_grad():\n",
    "        x = model(**tokenizer(x, return_tensors='pt')).pooler_output\n",
    "    return x\n",
    "\n",
    "# load the dictionary with the type of the features\n",
    "path = './data/face_recognition_preprocessed_data/dataset/event_dict_all.pkl'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    event_dict = pickle.load(f)\n",
    "    \n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/y_label.pt'\n",
    "x_face_full = torch.load(path)\n",
    "\n",
    "# load the dictionary with the type of face presented (event type)\n",
    "path = './data/face_recognition_preprocessed_data/dataset/target_trial_type.npy'\n",
    "t_dict = np.load(path)\n",
    "\n",
    "print(set(list(t_dict)))\n",
    "\n",
    "dict_event_id = {e: idx for idx, e in enumerate(set(list(t_dict)))}\n",
    "\n",
    "# face cls\n",
    "# with this dictionary we can get the type of the face\n",
    "# that were presented to the subject\n",
    "# than the event type is translated into a sentence which \n",
    "# describe the event\n",
    "# finally for each sentence I compute the embedding with the LLM\n",
    "# and I use this embedding as a feature to regress the signal\n",
    "# and to classify the class of the face that were presented\n",
    "# to the subject\n",
    "dict_element_face_type = {\"famous_new\": \"Initial presentation of famous face\",\n",
    "                            \"famous_second_early\": \"Immediate repeated presentation of famous face\",\n",
    "                            \"famous_second_late\": \"Delayed repeated presentation of famous face\",\n",
    "                            \"scrambled_new\": \"Initial presentation of scrambled face\",\n",
    "                            \"scrambled_second_early\": \"Immediate repeated presentation of scrambled face\",\n",
    "                            \"scrambled_second_late\": \"Delayed repeated presentation of scrambled face\",\n",
    "                            \"unfamiliar_new\": \"Initial presentation of unfamiliar face\",\n",
    "                            \"unfamiliar_second_early\": \"Immediate repeated presentation of unfamiliar face\",\n",
    "                            \"unfamiliar_second_late\": \"Delayed repeated presentation of unfamiliar face\",\n",
    "                            \"right_nonsym\": \"Right button press to indicated less symmetric face\",\n",
    "                            \"left_sym\": \"Left button press to indicated more symmetric face\",\n",
    "                            \"left_nonsym\": \"Left button press to indicated less symmetric face\",\n",
    "                            \"right_sym\": \"Right button press to indicated more symmetric face\",\n",
    "                            \"boundary\": \"The boundary mark of each run as the run data was merged\",\n",
    "                            \"famous\": \"Famous or famiiar face\",\n",
    "                            \"unfamiliar\": \"Unfamiliar face\",\n",
    "                            \"scrambled\": \"Scrambled face\"}\n",
    "\n",
    "\n",
    "dict_element_face_type_embedding = {\n",
    "    k: get_features(model, v) for k, v in dict_element_face_type.items()\n",
    "}\n",
    "\n",
    "# covert the label array into a torch tensor with the embedding\n",
    "# of the sentence that describe the event\n",
    "x_face_embedding = torch.cat([dict_element_face_type_embedding[i] for i in t_dict], dim = 0)\n",
    "\n",
    "print(x_face_embedding.shape)\n",
    "\n",
    "y_label_face = torch.tensor([dict_event_id[i] for i in t_dict]).type(torch.int8)\n",
    "\n",
    "print(y_label_face.shape)\n",
    "\n",
    "# torch.save(x_face_embedding, './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/x_face_embedding.pt')\n",
    "# torch.save(y_label_face, './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/y_label_face.pt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and preprocess the data to perform the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "dict_classes = {5: 0, 6: 1, 7: 2, 13: 3, 14: 4, 15: 5, 17: 6, 18: 7, 19: 8}\n",
    "classes = [5.0, 6.0, 7.0, 13.0, 14.0, 15.0, 17.0, 18.0, 19.0]\n",
    "number_of_samples_per_class = [2628, 1273, 1253, 2588, 1252, 1257, 2614, 1234, 1293]\n",
    "pick_number_of_samples_per_class = [2600, 1200, 1200, 2500, 1200, 1200, 2600, 1200, 1200]\n",
    "\n",
    "# signal to regrss\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/y_sample.pt'\n",
    "y_full = torch.load(path)\n",
    "\n",
    "# signal to regrss\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/y_label_face.pt'\n",
    "y_class = torch.load(path)\n",
    "\n",
    "# face cls\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/x_face_embedding.pt'\n",
    "x_face_full = torch.load(path)\n",
    "\n",
    "path = sorted(glob.glob('./data/face_recognition_preprocessed_data/tfr_dataset/full_dataset/data_per_class_*.pt'))\n",
    "\n",
    "print(path)\n",
    "\n",
    "conta = 0 \n",
    "conta_full = 0 # becase I pick the samples from the full dataset\n",
    "number_sample_per_class = 1000\n",
    "\n",
    "X = []\n",
    "x_face = []\n",
    "y_label_signal = []\n",
    "y_label_class = []\n",
    "\n",
    "for p, n_sample_class, n_sample_class_full in zip(path, pick_number_of_samples_per_class, number_of_samples_per_class):\n",
    "    print(p)\n",
    "    data = torch.load(p)\n",
    "    data = data[:number_sample_per_class].type(torch.float16)\n",
    "\n",
    "    # feature maps with the presence of the frequencies\n",
    "    X.append(data)\n",
    "\n",
    "    # face cls\n",
    "    x_face.append(x_face_full[conta_full:conta_full+number_sample_per_class])\n",
    "\n",
    "    # class\n",
    "    y_label_class.append(y_class[conta_full:conta_full+number_sample_per_class])\n",
    "\n",
    "    # signal to regress\n",
    "    y_label_signal.append(y_full[conta:conta+number_sample_per_class]*1e3)\n",
    "\n",
    "    conta = conta + n_sample_class\n",
    "    conta_full = conta_full + n_sample_class_full\n",
    "    \n",
    "\n",
    "X = torch.cat(X, dim = 0)\n",
    "x_face = torch.cat(x_face, dim = 0)\n",
    "y_label_signal = torch.cat(y_label_signal, dim = 0)\n",
    "y_label_class = torch.cat(y_label_class, dim = 0)\n",
    "\n",
    "print(X.shape, x_face.shape, y_label_signal.shape, y_label_class.shape)\n",
    "\n",
    "# torch.save(X, './data/face_recognition_preprocessed_data/tfr_dataset/X.pt')\n",
    "# torch.save(x_face, './data/face_recognition_preprocessed_data/tfr_dataset/x_face.pt')\n",
    "# torch.save(y_label_signal, './data/face_recognition_preprocessed_data/tfr_dataset/y_label_signal.pt')\n",
    "# torch.save(y_label_class, './data/face_recognition_preprocessed_data/tfr_dataset/y_label_class.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/X.pt'\n",
    "x = torch.load(path)\n",
    "x = x[:5000].type(torch.float32)\n",
    "\n",
    "torch.save(x, './data/face_recognition_preprocessed_data/tfr_dataset/X_5000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type               | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model                    | Architecture       | 11.3 M\n",
      "1 | criterion_classification | CrossEntropyLoss   | 0     \n",
      "2 | criterion_regression     | MSELoss            | 0     \n",
      "3 | huber_loss               | SmoothL1Loss       | 0     \n",
      "4 | classification_accuracy  | MulticlassAccuracy | 0     \n",
      "----------------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.118    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 3/125 [00:06<04:33,  2.24s/it, v_num=17, train_loss_step=0.702, train_loss_classification_step=0.608, train_loss_regression_step=0.0938, train_huber_loss_step=0.0445, train_acc_classification_step=0.844, val_loss=nan.0, val_loss_classification=nan.0, val_loss_regression=nan.0, val_huber_loss=nan.0, val_acc_classification=0.816, train_loss_epoch=1.660, train_loss_classification_epoch=1.460, train_loss_regression_epoch=0.200, train_huber_loss_epoch=0.0945, train_acc_classification_epoch=0.487]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/pytorch2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# number of classes\n",
    "NUMBER_CLASSES = 9\n",
    "\n",
    "#############################\n",
    "# load the data\n",
    "#############################\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/X.pt'\n",
    "x = torch.load(path)\n",
    "x = x[:5000].type(torch.float32)\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/x_face.pt'\n",
    "x_face = torch.load(path)\n",
    "x_face = x_face[:5000].type(torch.float32)\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/y_label_signal.pt'\n",
    "y_label_signal = torch.load(path)\n",
    "y_label_signal = y_label_signal[:5000].type(torch.float32)\n",
    "\n",
    "path = './data/face_recognition_preprocessed_data/tfr_dataset/y_label_class.pt'\n",
    "y_label_class = torch.load(path)\n",
    "y_label_class = y_label_class[:5000].type(torch.float32)\n",
    "\n",
    "# conversion for one hot encoding\n",
    "# dict_class = {5: 0, 6: 1, 7: 2, 13: 3, 14: 4, 15: 5, 17: 6, 18: 7, 19: 8}\n",
    "# y_label_class = torch.tensor([dict_class[y] for y in y_label_class.tolist()], dtype=torch.float32)\n",
    "\n",
    "# class label into one hot encoding\n",
    "y_label_class = torch.nn.functional.one_hot(y_label_class.to(torch.int64), num_classes=NUMBER_CLASSES).type(torch.float32)\n",
    "\n",
    "\n",
    "#############################\n",
    "# split the data\n",
    "#############################\n",
    "\n",
    "# dataset\n",
    "dataset = TensorDataset(x, x_face, y_label_signal, y_label_class)\n",
    "\n",
    "del x, x_face, y_label_signal, y_label_class\n",
    "\n",
    "# train, validation and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# train, validation and test dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "del train_set\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "del val_set\n",
    "\n",
    "#############################\n",
    "# train\n",
    "#############################\n",
    "\n",
    "# model\n",
    "model = Net()\n",
    "\n",
    "# # callbacks\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor='val_loss',\n",
    "#     dirpath='./models/face_recognition',\n",
    "#     filename='face_recognition-{epoch:02d}-{val_loss:.2f}',\n",
    "#     save_top_k=2,   \n",
    "#     mode='min',\n",
    "# )\n",
    "\n",
    "# early_stop_callback = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     min_delta=0.00,\n",
    "#     patience=5,\n",
    "#     verbose=True,\n",
    "#     mode='min'\n",
    "# )\n",
    "\n",
    "# trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='auto',\n",
    "    # callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    max_epochs=10\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# load the best model\n",
    "# model = Net.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uri_mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
