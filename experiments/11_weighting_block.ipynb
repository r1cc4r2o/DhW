{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class LinearWeightBlock(nn.Module):\n",
    "    def __init__(self, lenght_sequence, in_channels, n_head = 2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        ##################################################\n",
    "        # 1. Convolutional Block\n",
    "        ##################################################\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.conv_11 = nn.Conv1d(\n",
    "                                 in_channels=in_channels, \n",
    "                                 out_channels=in_channels//4, \n",
    "                                 kernel_size=3, \n",
    "                                 stride=1, \n",
    "                                 padding=1, \n",
    "                                 dilation=1, \n",
    "                                 groups=1, \n",
    "                                 bias=True\n",
    "                            )\n",
    "        self.conv_12 = nn.Conv1d(\n",
    "                                 in_channels=in_channels, \n",
    "                                 out_channels=in_channels//4,\n",
    "                                 kernel_size=3, \n",
    "                                 stride=1, \n",
    "                                 padding=1, \n",
    "                                 dilation=1, \n",
    "                                 groups=1, \n",
    "                                 bias=True\n",
    "                            )\n",
    "        self.conv_11_12 = nn.Conv1d(\n",
    "                                    in_channels=in_channels//4, \n",
    "                                    out_channels=in_channels//2, \n",
    "                                    kernel_size=3, \n",
    "                                    stride=2, \n",
    "                                    padding=1, \n",
    "                                    dilation=1, \n",
    "                                    groups=1, \n",
    "                                    bias=True\n",
    "                            )\n",
    "\n",
    "        self.conv_21 = nn.Conv1d(\n",
    "                                in_channels=in_channels//2, \n",
    "                                out_channels=in_channels//2, \n",
    "                                kernel_size=3, \n",
    "                                stride=1, \n",
    "                                padding=1, \n",
    "                                dilation=1, \n",
    "                                groups=1, \n",
    "                                bias=True\n",
    "                            )\n",
    "        self.conv_22 = nn.Conv1d(\n",
    "                                in_channels=in_channels//2, \n",
    "                                out_channels=in_channels//2, \n",
    "                                kernel_size=3, \n",
    "                                stride=1, \n",
    "                                padding=1, \n",
    "                                dilation=1, \n",
    "                                groups=1, \n",
    "                                bias=True\n",
    "                            )\n",
    "        self.conv_21_22 = nn.Conv1d(\n",
    "                                in_channels=in_channels//2, \n",
    "                                out_channels=in_channels//2, \n",
    "                                kernel_size=3, \n",
    "                                stride=2, \n",
    "                                padding=1, \n",
    "                                dilation=1, \n",
    "                                groups=1, \n",
    "                                bias=True\n",
    "                            )\n",
    "\n",
    "        # layer norm\n",
    "        self.lenght_sequence = lenght_sequence\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(self.lenght_sequence)\n",
    "        self.layer_norm_11_12 = nn.LayerNorm(self.lenght_sequence//2)\n",
    "\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.lenght_sequence//2)\n",
    "        self.layer_norm_21_22 = nn.LayerNorm(self.lenght_sequence//4)\n",
    "\n",
    "        ##################################################\n",
    "        # 2. Linear Weighting Block\n",
    "        ##################################################\n",
    "        \n",
    "        self.dim_embedding = 300 \n",
    "        c = 1\n",
    "        \n",
    "        self.linear = nn.Linear(self.dim_embedding, self.dim_embedding//8 + c)\n",
    "        \n",
    "        ##################################################\n",
    "        # 3. Transformer Encoder Block\n",
    "        ##################################################\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.dim_embedding//8+ c, nhead=n_head), num_layers=2\n",
    "        )\n",
    "        \n",
    "        ##################################################\n",
    "        # 4. Residual Bilinear Block\n",
    "        ##################################################\n",
    "\n",
    "        self.bilinear = nn.Bilinear(self.dim_embedding//8 + c, self.dim_embedding//8 + c, self.dim_embedding//8 + c)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "        ##################################################\n",
    "        # - Layer Norm\n",
    "        # - GELU\n",
    "        ##################################################\n",
    "        \n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(self.dim_embedding//8 + c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # down sampling 1\n",
    "        x_1 = self.layer_norm_1(self.gelu(self.conv_11(x)))\n",
    "        x_2 = self.layer_norm_1(self.gelu(self.conv_12(x)))\n",
    "        x = self.layer_norm_11_12(self.conv_11_12(x_1 + x_2))\n",
    "\n",
    "        # down sampling 2\n",
    "        x_1 = self.layer_norm_2(self.gelu(self.conv_21(x)))\n",
    "        x_2 = self.layer_norm_2(self.gelu(self.conv_22(x)))\n",
    "        x = self.layer_norm_21_22(self.conv_21_22(x_1 + x_2))\n",
    "\n",
    "        # linear weighting block\n",
    "        x = self.linear(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # transformer encoder block\n",
    "        x_0 = self.transformer_encoder(x)\n",
    "\n",
    "        # residual bilinear block\n",
    "        x = self.bilinear(x_0 + self.dropout(x_0), x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, lenght_sequence = 1200, in_channels = 235, n_head = 2, n_class = 15) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.linear_weight_block = LinearWeightBlock(lenght_sequence, in_channels, n_head)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=- 1)\n",
    "\n",
    "        self.linear_transformation = nn.Linear(((lenght_sequence//4)//8 + 1) * (in_channels//2), lenght_sequence//32)\n",
    "        # self.linear_transformation = nn.Linear((lenght_sequence//8 + 1) * (in_channels//2), lenght_sequence//32)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(lenght_sequence//32)\n",
    "\n",
    "        self.mlp_subject_info = nn.Sequential(\n",
    "            nn.Linear(4, lenght_sequence//32),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(lenght_sequence//32),\n",
    "            nn.Linear(lenght_sequence//32, lenght_sequence//32)\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(lenght_sequence//32, lenght_sequence//16),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(lenght_sequence//16),\n",
    "            nn.Linear(lenght_sequence//16, n_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, subject_info):\n",
    "        x = self.linear_weight_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_transformation(x)\n",
    "        x = self.layer_norm(x)\n",
    "        # adding subject info as a shift\n",
    "        x = x + self.mlp_subject_info(subject_info)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LitClassifier(pl.LightningModule):\n",
    "    def __init__(self, lenght_sequence = 1200, in_channels = 235, n_head = 2, n_class = 15) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = Classifier(lenght_sequence, in_channels, n_head, n_class)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=n_class)\n",
    "        self.f1 = torchmetrics.F1Score(task=\"multiclass\",num_classes=n_class)\n",
    "        self.precision = torchmetrics.Precision(task=\"multiclass\",num_classes=n_class)\n",
    "        self.recall = torchmetrics.Recall(task=\"multiclass\",num_classes=n_class)\n",
    "\n",
    "        \n",
    "    def forward(self, x, subject_info):\n",
    "        return self.model(x, subject_info)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, subject_info, y = batch\n",
    "        \n",
    "        # y = y.long()\n",
    "\n",
    "        y_hat = self(x, subject_info)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        y = torch.argmax(y, dim=-1)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", self.accuracy(y_hat, y))\n",
    "        self.log(\"train_f1\", self.f1(y_hat, y))\n",
    "        self.log(\"train_precision\", self.precision(y_hat, y))\n",
    "        self.log(\"train_recall\", self.recall(y_hat, y))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, subject_info, y = batch\n",
    "\n",
    "        # y = y.long()\n",
    "        \n",
    "        y_hat = self(x, subject_info)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        y = torch.argmax(y, dim=-1)\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", self.accuracy(y_hat, y))\n",
    "        self.log(\"val_f1\", self.f1(y_hat, y))\n",
    "        self.log(\"val_precision\", self.precision(y_hat, y))\n",
    "        self.log(\"val_recall\", self.recall(y_hat, y))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     x, subject_info, y = batch\n",
    "\n",
    "    #     # y = y.long()\n",
    "        \n",
    "    #     y_hat = self(x, subject_info)\n",
    "    #     loss = self.loss(y_hat, y)\n",
    "\n",
    "    #     y = torch.argmax(y, dim=-1)\n",
    "\n",
    "    #     self.log(\"test_loss\", loss)\n",
    "    #     self.log(\"test_acc\", self.accuracy(y_hat, y))\n",
    "    #     self.log(\"test_f1\", self.f1(y_hat, y))\n",
    "    #     self.log(\"test_precision\", self.precision(y_hat, y))\n",
    "    #     self.log(\"test_recall\", self.recall(y_hat, y))\n",
    "        \n",
    "    #     return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load('./data/emotion_recognition_preprocessed_data/preprocessed/data.pt')\n",
    "info = torch.load('./data/emotion_recognition_preprocessed_data/preprocessed/info.pt').type(torch.float32)\n",
    "labels = torch.load('./data/emotion_recognition_preprocessed_data/preprocessed/labels.pt').type(torch.long)\n",
    "\n",
    "\n",
    "labels = torch.nn.functional.one_hot(labels, num_classes=15).type(torch.float32)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "BACH_SIZE = 256\n",
    "\n",
    "dataset = TensorDataset(data, info, labels)\n",
    "\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = int(0.1 * len(dataset))\n",
    "# test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BACH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BACH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BACH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BACH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BACH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | model     | Classifier          | 794 K \n",
      "1 | loss      | CrossEntropyLoss    | 0     \n",
      "2 | accuracy  | MulticlassAccuracy  | 0     \n",
      "3 | f1        | MulticlassF1Score   | 0     \n",
      "4 | precision | MulticlassPrecision | 0     \n",
      "5 | recall    | MulticlassRecall    | 0     \n",
      "--------------------------------------------------\n",
      "794 K     Trainable params\n",
      "0         Non-trainable params\n",
      "794 K     Total params\n",
      "3.177     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca697bc8d834a4f8a9f3ddb75fc0150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickbook/mambaforge/envs/uri_mne/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:480: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/rickbook/mambaforge/envs/uri_mne/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/rickbook/mambaforge/envs/uri_mne/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/rickbook/mambaforge/envs/uri_mne/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc473acc09394e63ad5b43dcbb145693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eee1e93d15458496cf47e5a650b322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ba3a7a69db44f6b0e5e5985ea4c33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6b4cb2760541b693de17d00d9c35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 2.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37616b17a314f03b96c8d8e0db7abbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77bb2e8bb5748678012e31bb8aa5f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822e630e9bef4bbda6f275601c38cd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 2.709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a861229a989b46089c2e722f7eb51d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5110e17c36a46828f72455e7e5f0c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51c151ba59d4a8787201305b1426fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01787e77055a45e08cd5cc3585ff87fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "net = LitClassifier().cuda()\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping('val_loss', patience=150, verbose=True, mode='min')\n",
    "modelCheckPoint = ModelCheckpoint(monitor='val_loss', save_top_k=1, mode='min', dirpath='./models', filename='model-{epoch:02d}-{val_loss:.2f}')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator='auto', callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(net, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 312), started 0:03:03 ago. (Use '!kill 312' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-47b01ae455cc6fd0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-47b01ae455cc6fd0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !kill 600\n",
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uri_mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
